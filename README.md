# Music Player Using Emotion Detection

## Project Description

The **Emotion-Detecting Music Player** is an intelligent application that uses facial emotion recognition to create a personalized music experience. By detecting the user's emotional state through a camera, the player automatically selects and plays music that matches or enhances the detected mood.

### âœ¨ New Features (v2.0)

- **ğŸ¯ Emotion Stability System**: Prevents rapid emotion jumping with confidence-based validation
- **âš™ï¸ Configurable Settings**: Adjust sensitivity and behavior via `config.json`
- **ğŸµ Multi-Format Support**: Plays MP3, WAV, OGG, and FLAC files
- **ğŸ“Š Confidence Display**: Real-time emotion confidence visualization
- **ğŸ”Š Volume Control**: Adjustable volume slider in UI
- **ğŸŒ Cross-Platform**: Works on Windows, macOS, and Linux
- **ğŸ“ Enhanced Logging**: Detailed logs for debugging and analytics
- **ğŸš€ Improved Training**: Better model training with data augmentation and callbacks
- **â˜ï¸ Colab Notebook**: Train models in the cloud with GPU acceleration

## Features

- **Real-Time Emotion Detection:** 
  - Uses camera to capture and analyze facial expressions
  - Supports 7 emotions: Happy, Sad, Fear, Disgust, Surprise, Angry, and Neutral
  - Confidence-based validation prevents false detections
  
- **Smart Emotion Smoothing:**
  - Prevents rapid emotion changes with configurable thresholds
  - Requires minimum confidence (default 60%) before changing emotion
  - Enforces minimum duration (default 5 seconds) for stable experience
  
- **Mood-Based Music Playback:** 
  - Automatically selects music matching detected emotion
  - Smart track selection avoids recent repeats
  - Smooth fade transitions between tracks
  
- **Enhanced User Interface:** 
  - Confidence percentage and progress bar
  - Current track display
  - Volume control slider
  - Manual emotion selection
  - Pause/play and skip controls

## Installation

### Prerequisites

- Python 3.8 or higher
- Webcam or camera
- Audio output device

### Steps

1. Navigate to the desired directory using the shell/command prompt.

2. Clone the repository:
    ```sh
    git clone https://github.com/Seethalekshmy/Saaram.git
    ```

3. Navigate to the project directory:
    ```sh
    cd Saaram
    ```

4. Install the required dependencies:
    
    **Windows:**
    ```sh
    pip install -r requirements.txt
    ```
    
    **macOS:**
    ```sh
    pip install -r requirements.txt
    # If pygame installation fails:
    brew install sdl2 sdl2_image sdl2_mixer sdl2_ttf
    pip install pygame --pre
    ```
    
    **Linux:**
    ```sh
    sudo apt-get install python3-opencv python3-pygame
    pip install -r requirements.txt
    ```

5. Download the pre-trained model: [Click here](https://drive.google.com/file/d/1M52BHKj0jHqzLb_hWR32xqRU-RADDikV/view?usp=sharing)
   - Place `model.weights.h5` in the project root directory
   - **OR** train your own model (see Training section below)

6. Download the music library: [Click here](https://drive.google.com/drive/folders/1vqBbMODw54qFj-yE4OlC9n_98m6mT7_L?usp=sharing)
   - Extract the `Music` folder to the project root directory
   - **OR** add your own music (see Music Organization section)

## Usage

1. Ensure `model.weights.h5` is in the project directory

2. Run the main script:
    ```sh
    python main.py
    ```

3. The application will:
   - Open a camera window showing real-time emotion detection
   - Display the UI with emotion and confidence information
   - Automatically play music based on detected emotion

4. Controls:
   - **Play/Pause**: Toggle music playback
   - **Next**: Skip to next track in current emotion
   - **Volume Slider**: Adjust playback volume
   - **Emotion Dropdown**: Manually select emotion
   - **Quit**: Close application
   - **Press 'Q'** in camera window to exit

## Music Organization

Organize your music files in emotion-based folders:

```
Music/
â”œâ”€â”€ Angry/      # Intense, aggressive music
â”œâ”€â”€ Disgust/    # Dark, heavy music
â”œâ”€â”€ Fear/       # Suspenseful, tense music
â”œâ”€â”€ Happy/      # Upbeat, cheerful music
â”œâ”€â”€ Sad/        # Melancholic, slow music
â”œâ”€â”€ Surprise/   # Energetic, unexpected music
â””â”€â”€ Neutral/    # Calm, ambient music
```

**Supported formats:** MP3, WAV, OGG, FLAC

## Configuration

Customize the application behavior by editing `config.json`:

```json
{
  "emotion_detection": {
    "confidence_threshold": 0.6,      // Adjust emotion sensitivity
    "minimum_emotion_duration": 5.0,  // Prevent rapid changes
    "smoothing_window_size": 15       // Detection smoothing
  },
  "audio": {
    "default_volume": 0.7,            // Initial volume (0.0 - 1.0)
    "fade_duration_ms": 1000          // Fade transition time
  }
}
```

See [CONFIGURATION.md](CONFIGURATION.md) for detailed configuration guide.

## Training Your Own Model

### Option 1: Local Training

1. Ensure you have the training dependencies:
    ```sh
    pip install matplotlib deeplake
    ```

2. Run the training script:
    ```sh
    python train.py
    ```

3. The script will:
   - Download the FER2013 dataset automatically
   - Train with data augmentation and callbacks
   - Save the best model as `model.weights.h5`
   - Generate training plots

### Option 2: Google Colab (Recommended)

1. Upload `emotion_training.ipynb` to Google Colab

2. Enable GPU acceleration:
   - Runtime â†’ Change runtime type â†’ GPU

3. Run all cells to train the model

4. Download `model.weights.h5` when training completes

**Benefits of Colab:**
- Free GPU acceleration (faster training)
- No local setup required
- Interactive visualization
- Easy experimentation

## Directory Structure

```
Saaram/
â”œâ”€â”€ Modules/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ music.py              # Music playback engine
â”‚   â”œâ”€â”€ UI.py                 # User interface
â”‚   â”œâ”€â”€ emotion_smoother.py   # Emotion stability system
â”‚   â”œâ”€â”€ config_manager.py     # Configuration handler
â”‚   â””â”€â”€ logger.py             # Logging system
â”œâ”€â”€ Music/                    # Emotion-based music folders
â”‚   â”œâ”€â”€ Angry/
â”‚   â”œâ”€â”€ Disgust/
â”‚   â”œâ”€â”€ Fear/
â”‚   â”œâ”€â”€ Happy/
â”‚   â”œâ”€â”€ Sad/
â”‚   â”œâ”€â”€ Surprise/
â”‚   â””â”€â”€ Neutral/
â”œâ”€â”€ config.json               # Configuration file
â”œâ”€â”€ main.py                   # Main application
â”œâ”€â”€ train.py                  # Local training script
â”œâ”€â”€ emotion_training.ipynb    # Colab training notebook
â”œâ”€â”€ model.weights.h5          # Trained model weights
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ CONFIGURATION.md          # Configuration guide
â””â”€â”€ README.md                 # This file
```

## Troubleshooting

### Emotion Changes Too Frequently
- Increase `confidence_threshold` in `config.json` (try 0.7)
- Increase `minimum_emotion_duration` (try 7 seconds)

### Emotion Doesn't Change
- Decrease `confidence_threshold` (try 0.5)
- Ensure good lighting on your face
- Face the camera directly

### Camera Not Working
- Check camera permissions in system settings
- Try different `device_index` in `config.json` (0, 1, 2, etc.)

### No Music Playing
- Verify music files are in correct folders
- Check supported formats (MP3, WAV, OGG, FLAC)
- Check volume slider in UI

### Performance Issues
- Reduce camera resolution in `config.json`
- Close other applications
- Use a simpler model (reduce epochs during training)

See [CONFIGURATION.md](CONFIGURATION.md) for more troubleshooting tips.

## Technical Details

### Emotion Detection Model
- **Architecture**: Convolutional Neural Network (CNN)
- **Input**: 48x48 grayscale images
- **Output**: 7 emotion classes
- **Dataset**: FER2013 (35,887 training images)
- **Accuracy**: ~65-70% on validation set

### Emotion Smoothing Algorithm
- **Rolling Window**: Analyzes last N detections
- **Confidence Calculation**: Percentage of window matching emotion
- **Transition Dampening**: Minimum duration requirement
- **History Tracking**: Logs all emotion changes

## Performance Tips

1. **Better Accuracy**:
   - Ensure good, even lighting
   - Face camera directly
   - Maintain neutral background
   - Avoid extreme angles

2. **Faster Response**:
   - Lower `minimum_emotion_duration`
   - Reduce `smoothing_window_size`
   - Lower `confidence_threshold`

3. **More Stability**:
   - Increase `confidence_threshold`
   - Increase `minimum_emotion_duration`
   - Increase `smoothing_window_size`

## Contributing

Contributions are welcome! Areas for improvement:
- Additional emotion classes
- Better model architectures
- UI themes and customization
- Spotify/streaming service integration
- Mobile app version

## License

This project is licensed under the GPL-3.0 License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- FER2013 dataset for emotion recognition training
- OpenCV for computer vision capabilities
- TensorFlow/Keras for deep learning
- Pygame for audio playback

---

<div align="center">

**Made with â¤ï¸ for a project expo**

[Report Bug](https://github.com/Seethalekshmy/Saaram/issues) Â· [Request Feature](https://github.com/Seethalekshmy/Saaram/issues)

</div>
